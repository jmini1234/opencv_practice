{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "opensw_final.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jmini1234/opencv_practice/blob/master/opensw_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhoqHrGgtWA8",
        "colab_type": "text"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nR9W_r_po4so",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "class Loader(Dataset):\n",
        "\n",
        "    def _check_exists(self):\n",
        "        return os.path.exists(os.path.join(self.VOC2012_dir, \"JPEGImages\" )) and \\\n",
        "               os.path.exists(os.path.join(self.VOC2012_dir, \"SegmentationObject\"))\n",
        "\n",
        "    def VOCdataloader(self, index):\n",
        "        # Load Image\n",
        "        path1 = os.path.join(self.VOC2012_dir, \"JPEGImages\", self.imgnames[index].split(\"\\n\")[0] + \".jpg\")\n",
        "        img1 = cv2.imread(path1, cv2.IMREAD_COLOR)\n",
        "        img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n",
        "        ori_img = self.transform(img1)\n",
        "        ori_img = np.asarray(ori_img)\n",
        "\n",
        "        # Load mask(=label)\n",
        "        path2 = os.path.join(self.VOC2012_dir, \"SegmentationClass\", self.imgnames[index].split(\"\\n\")[0] + \".png\")\n",
        "        img2 = cv2.imread(path2, cv2.IMREAD_COLOR)\n",
        "        img2 = cv2.cvtColor(img2, cv2.COLOR_BGR2RGB)\n",
        "        seg_img = cv2.resize(img2, dsize=(self.resize, self.resize), interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "        label = np.zeros((self.resize, self.resize), dtype=np.uint8)\n",
        "        for i in range(self.resize):\n",
        "            for j in range(self.resize):\n",
        "                label[i][j] = self.cls[tuple(seg_img[i, j, :])]\n",
        "\n",
        "        t_ori_img = torch.from_numpy(ori_img)\n",
        "        label1 = torch.Tensor(label)\n",
        "\n",
        "        return t_ori_img, label1\n",
        "\n",
        "    def __init__(self, VOC2012_dir, flag, resize, transforms):\n",
        "        self.VOC2012_dir = VOC2012_dir\n",
        "        self.resize = resize\n",
        "        self.transform = transforms\n",
        "        self.flag = flag\n",
        "\n",
        "        with open(self.VOC2012_dir + \"/ImageSets/Segmentation/trainval.txt\", 'r') as f:\n",
        "            self.lines = f.readlines()\n",
        "\n",
        "        # Split Test and Train\n",
        "        self.fold = int(len(self.lines)*0.2)\n",
        "\n",
        "        if self.flag == 'train':\n",
        "            # self.imgnames = self.lines[:50] # Tip : you can adjust the number of images and run the quickly during debugging.\n",
        "            self.imgnames = self.lines[self.fold:]\n",
        "\n",
        "        else:\n",
        "            # self.imgnames = self.lines[50:60] # Tip : you can adjust the number of images and run the quickly during debugging.\n",
        "            self.imgnames = self.lines[:self.fold]\n",
        "\n",
        "        self.cls = {(0, 0, 0): 0, (128, 0, 0): 1, (0, 128, 0): 2,  # 0:background, 1:aeroplane, 2:bicycle\n",
        "               (128, 128, 0): 3, (0, 0, 128): 4, (128, 0, 128): 5,  # 3:bird, 4:boat, 5:bottle\n",
        "               (0, 128, 128): 6, (128, 128, 128): 7, (64, 0, 0): 8,  # 6:bus, 7:car, 8:cat\n",
        "               (192, 0, 0): 9, (64, 128, 0): 10, (192, 128, 0): 11,  # 9:chair, 10:cow, 11:diningtable\n",
        "               (64, 0, 128): 12, (192, 0, 128): 13, (64, 128, 128): 14,  # 12:dog, 13:horse, 14:motorbike\n",
        "               (192, 128, 128): 15, (0, 64, 0): 16, (128, 64, 0): 17,  # 15:person, 16:pottedplant, 17:sheep\n",
        "               (0, 192, 0): 18, (128, 192, 0): 19, (0, 64, 128): 20,  # 18:sofa, 19:train, 20:tvmonitor\n",
        "               (224, 224, 192): 21}  # edge\n",
        "\n",
        "        if not self._check_exists():\n",
        "            raise RuntimeError(\"Dataset not found.\")\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgnames)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        if torch.is_tensor(index):\n",
        "            index = index.tolist()\n",
        "        images, masks = self.VOCdataloader(index)\n",
        "\n",
        "        return images, masks\n",
        "\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VnU-SNB5tJVo",
        "colab_type": "text"
      },
      "source": [
        "# UNET-skeleton\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bq0tFcgIz5Xo",
        "colab_type": "text"
      },
      "source": [
        "# modules_skeleton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vF4bo12wtI5H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "###########################################################################\n",
        "    # Question 3 : Implement the train/test module.\n",
        "    # Understand train/test codes in Practice Lecture 14, and fill in the blanks.(30 points)\n",
        "def train_model(trainloader, model, criterion, optimizer,scheduler, device):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    total_step = len(trainloader)\n",
        "\n",
        "    for i, (inputs, labels) in enumerate(trainloader):\n",
        "        from datetime import datetime\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device=device, dtype=torch.int64)\n",
        "        criterion = criterion.cuda()\n",
        "        #1. Get the output out of model, and Get the loss\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # 3. optimizer\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "        # 4. backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "          print(\"Step [{}/{}], Train_Loss: {:.4f}\"\n",
        "                .format(i+1, total_step, train_loss / (i + 1)))\n",
        "\n",
        "def accuracy_check(label, pred):\n",
        "    ims = [label, pred]\n",
        "    np_ims = []\n",
        "    for item in ims:\n",
        "        item = np.array(item)\n",
        "        np_ims.append(item)\n",
        "    compare = np.equal(np_ims[0], np_ims[1])\n",
        "    accuracy = np.sum(compare)\n",
        "    return accuracy / len(np_ims[0].flatten())\n",
        "\n",
        "def accuracy_check_for_batch(labels, preds, batch_size):\n",
        "    total_acc = 0\n",
        "    for i in range(batch_size):\n",
        "        total_acc += accuracy_check(labels[i], preds[i])\n",
        "    return total_acc/batch_size\n",
        "\n",
        "def get_loss_train(model, trainloader, criterion, device):\n",
        "\n",
        "    total_step = len(trainloader)\n",
        "    model.eval()\n",
        "    total_acc = 0\n",
        "    total_loss = 0\n",
        "    for batch, (inputs, labels) in enumerate(trainloader):\n",
        "        with torch.no_grad():\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device = device, dtype = torch.int64)\n",
        "            inputs = inputs.float()\n",
        "            ##########################################\n",
        "            ############# fill in here (5 points) -> (same as validation, just printing loss)\n",
        "            ####### Hint :\n",
        "            ####### Get the output out of model, and Get the Loss\n",
        "            ####### Think what's different from the above\n",
        "            #########################################\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs,labels)\n",
        "\n",
        "            \n",
        "            outputs = np.transpose(outputs.cpu(), (0,2,3,1))\n",
        "            preds = torch.argmax(outputs, dim=3).float()\n",
        "            acc = accuracy_check_for_batch(labels.cpu(), preds.cpu(), inputs.size()[0])\n",
        "            total_acc += acc\n",
        "            total_loss += loss.cpu().item()\n",
        "\n",
        "            if (batch + 1) % 10 == 0:\n",
        "              print(\"Step [{}/{}], Loss: {:.4f}\"\n",
        "                        .format(batch+1, total_step, total_loss / (batch + 1)))\n",
        "              \n",
        "    return total_acc/(batch+1), total_loss/(batch+1)\n",
        "\n",
        "from PIL import Image\n",
        "def val_model(model, valloader, criterion, device, dir):\n",
        "    total_step = len(valloader)\n",
        "\n",
        "    cls_invert = {0: (0, 0, 0), 1: (128, 0, 0), 2: (0, 128, 0),  # 0:background, 1:aeroplane, 2:bicycle\n",
        "                  3: (128, 128, 0), 4: (0, 0, 128), 5: (128, 0, 128),  # 3:bird, 4:boat, 5:bottle\n",
        "                  6: (0, 128, 128), 7: (128, 128, 128), 8: (64, 0, 0),  # 6:bus, 7:car, 8:cat\n",
        "                  9: (192, 0, 0), 10: (64, 128, 0), 11: (192, 128, 0),  # 9:chair, 10:cow, 11:diningtable\n",
        "                  12: (64, 0, 128), 13: (192, 0, 128), 14: (64, 128, 128),  # 12:dog, 13:horse, 14:motorbike\n",
        "                  15: (192, 128, 128), 16: (0, 64, 0), 17: (128, 64, 0),  # 15:person, 16:pottedplant, 17:sheep\n",
        "                  18: (0, 192, 0), 19: (128, 192, 0), 20: (0, 64, 128),  # 18:sofa, 19:train, 20:tvmonitor\n",
        "                  21: (224, 224, 192)}\n",
        "    total_val_loss = 0\n",
        "    total_val_acc = 0\n",
        "    n=0\n",
        "\n",
        "    for batch, (inputs, labels) in enumerate(valloader):\n",
        "        with torch.no_grad():\n",
        "\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device=device, dtype=torch.int64)\n",
        "            ##########################################\n",
        "            ############# fill in here (5 points) -> (validation)\n",
        "            ####### Hint :\n",
        "            ####### Get the output out of model, and Get the Loss\n",
        "            ####### Think what's different from the above\n",
        "            #########################################\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs,labels)\n",
        "\n",
        "\n",
        "            outputs = np.transpose(outputs.cpu(), (0, 2, 3, 1))\n",
        "            preds = torch.argmax(outputs, dim=3).float()\n",
        "\n",
        "            acc = accuracy_check_for_batch(labels.cpu(), preds.cpu(), inputs.size()[0])\n",
        "            total_val_acc += acc\n",
        "            total_val_loss += loss.cpu().item()\n",
        "\n",
        "            if (batch + 1) % 10 == 0:\n",
        "              print(\"Step [{}/{}], Val_Loss: {:.4f}\"\n",
        "                        .format(batch+1,total_step, total_val_loss / (batch + 1)))\n",
        "            \n",
        "            for i in range(preds.shape[0]):\n",
        "                temp = preds[i].cpu().data.numpy()\n",
        "                temp_l = labels[i].cpu().data.numpy()\n",
        "                temp_rgb = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
        "                temp_label = np.zeros((temp.shape[0], temp.shape[1], 3))\n",
        "\n",
        "                for j in range(temp.shape[0]):\n",
        "                    for k in range(temp.shape[1]):\n",
        "                        ##########################################\n",
        "                        ############# fill in here (10 points)\n",
        "                        ####### Hint :\n",
        "                        ####### convert segmentation mask into r,g,b (both for image and predicted result)\n",
        "                        ####### image should become temp_rgb, result should become temp_label\n",
        "                        ####### You should use cls_invert[]\n",
        "                        #########################################\n",
        "                        temp_rgb[j][k] = cls_invert[temp[j][k]]\n",
        "                        temp_label[j][k] = cls_invert[temp_l[j][k]]\n",
        "\n",
        "                img = inputs[i].cpu()\n",
        "                img = np.transpose(img, (2, 1, 0))\n",
        "\n",
        "                img_print = Image.fromarray(np.uint8(temp_label))\n",
        "                mask_print = Image.fromarray(np.uint8(temp_rgb))\n",
        "\n",
        "                img_print.save(dir + str(n) + 'label' + '.png')\n",
        "                mask_print.save(dir + str(n) + 'result' + '.png')\n",
        "\n",
        "                n += 1\n",
        "\n",
        "    return total_val_acc/(batch+1), total_val_loss/(batch+1)"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Yz_WqXjNRKX",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FuF89RZ21LOj",
        "colab_type": "text"
      },
      "source": [
        "# main_skeleton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HBKrLrRU4UF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e0966452-962d-4c6b-aa3b-5030c70c78ee"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8ZLD49gFxNI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f3b709f-fa29-404d-85a0-4828c3bff892"
      },
      "source": [
        "!pip install d2l==0.13.2 -f https://d2l.ai/whl.html # installing d2l\n",
        "!pip install -U mxnet-cu101mkl==1.6.0  # updating mxnet to at least v1.6"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in links: https://d2l.ai/whl.html\n",
            "Requirement already satisfied: d2l==0.13.2 in /usr/local/lib/python3.6/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from d2l==0.13.2) (1.18.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from d2l==0.13.2) (1.0.5)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from d2l==0.13.2) (1.0.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from d2l==0.13.2) (3.2.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.13.2) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->d2l==0.13.2) (2.8.1)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.13.2) (5.2.2)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.13.2) (4.7.5)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.13.2) (7.5.1)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.13.2) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.13.2) (5.6.1)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->d2l==0.13.2) (4.10.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.13.2) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.13.2) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->d2l==0.13.2) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->d2l==0.13.2) (1.12.0)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.13.2) (4.5.3)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.13.2) (4.6.3)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.13.2) (5.0.7)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.13.2) (5.3.4)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.13.2) (0.8.3)\n",
            "Requirement already satisfied: traitlets>=4.2.1 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.13.2) (4.3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.13.2) (2.11.2)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->d2l==0.13.2) (0.2.0)\n",
            "Requirement already satisfied: qtpy in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.13.2) (1.9.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.13.2) (2.1.3)\n",
            "Requirement already satisfied: pyzmq>=17.1 in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->d2l==0.13.2) (19.0.1)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.13.2) (3.5.1)\n",
            "Requirement already satisfied: ipython>=4.0.0; python_version >= \"3.3\" in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->d2l==0.13.2) (5.5.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->d2l==0.13.2) (1.0.18)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.13.2) (0.6.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.13.2) (3.1.5)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.13.2) (1.4.2)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.13.2) (0.8.4)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.13.2) (0.3)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->d2l==0.13.2) (0.4.4)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat->notebook->jupyter->d2l==0.13.2) (2.6.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->d2l==0.13.2) (0.6.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from traitlets>=4.2.1->notebook->jupyter->d2l==0.13.2) (4.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2->notebook->jupyter->d2l==0.13.2) (1.1.1)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l==0.13.2) (0.8.1)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l==0.13.2) (0.7.5)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l==0.13.2) (47.3.1)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython>=4.0.0; python_version >= \"3.3\"->ipywidgets->jupyter->d2l==0.13.2) (4.8.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->d2l==0.13.2) (0.2.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.13.2) (20.4)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach->nbconvert->jupyter->d2l==0.13.2) (0.5.1)\n",
            "Requirement already up-to-date: mxnet-cu101mkl==1.6.0 in /usr/local/lib/python3.6/dist-packages (1.6.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.20.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101mkl==1.6.0) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101mkl==1.6.0) (0.8.4)\n",
            "Requirement already satisfied, skipping upgrade: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.6/dist-packages (from mxnet-cu101mkl==1.6.0) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101mkl==1.6.0) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101mkl==1.6.0) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101mkl==1.6.0) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.20.0->mxnet-cu101mkl==1.6.0) (2.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2ximNbMFuW_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from d2l import mxnet as d2l\n",
        "from mxnet import gluon, image, np, npx\n",
        "import os\n",
        "\n",
        "npx.set_np()"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8534Z-9iF1qf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "d2l.DATA_HUB['voc2012'] = (d2l.DATA_URL + 'VOCtrainval_11-May-2012.tar',\n",
        "                           '4e443f8a2eca6b1dac8a6c57641b67dd40621a49')\n",
        "\n",
        "voc_dir = d2l.download_extract('voc2012', 'VOCdevkit/VOC2012')"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pog5zZTqnHly",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NA5de3tltIj7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "###########################################################################\n",
        "# Question 1 : Implement the UNet model code.\n",
        "# Understand architecture of the UNet in practice lecture 15 -> slides 5-6 (30 points)\n",
        "\n",
        "def conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),  # 3은 kernel size\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "\n",
        "class Unet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(Unet, self).__init__()\n",
        "\n",
        "        ########## fill in the blanks (Hint : check out the channel size in practice lecture 15 ppt slides 5-6)\n",
        "        self.convDown1 = conv(in_channels, 64)\n",
        "        self.convDown2 = conv(64, 128)\n",
        "        self.convDown3 = conv(128, 256)\n",
        "        self.convDown4 = conv(256, 512)\n",
        "        self.convDown5 = conv(512, 1024)\n",
        "        self.maxpool = nn.MaxPool2d(2, stride=2)\n",
        "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "        self.convUp4 = conv(1536, 512)\n",
        "        self.convUp3 = conv(768, 256)\n",
        "        self.convUp2 = conv(384, 128)\n",
        "        self.convUp1 = conv(192, 64)\n",
        "        self.convUp_fin = nn.Conv2d(64, out_channels, 1)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.convDown1(x)\n",
        "\n",
        "        x = self.maxpool(conv1)\n",
        "        conv2 = self.convDown2(x)\n",
        "\n",
        "        x = self.maxpool(conv2)\n",
        "        conv3 = self.convDown3(x)\n",
        "\n",
        "        x = self.maxpool(conv3)\n",
        "        conv4 = self.convDown4(x)\n",
        "\n",
        "        x = self.maxpool(conv4)\n",
        "        conv5 = self.convDown5(x)\n",
        "\n",
        "        x = self.upsample(conv5)\n",
        "        #######fill in here ####### hint : concatenation (Practice Lecture slides 6p)\n",
        "        x=torch.cat([conv4,x],dim=1)\n",
        "        x = self.convUp4(x)\n",
        "\n",
        "        x = self.upsample(x)\n",
        "        #######fill in here ####### hint : concatenation (Practice Lecture slides 6p)\n",
        "        x=torch.cat([conv3,x],dim=1)\n",
        "        x = self.convUp3(x)\n",
        "       \n",
        "        x = self.upsample(x)\n",
        "        #######fill in here ####### hint : concatenation (Practice Lecture slides 6p)\n",
        "        x=torch.cat([conv2,x],dim=1)\n",
        "        x = self.convUp2(x)\n",
        "    \n",
        "        x = self.upsample(x)\n",
        "        #######fill in here ####### hint : concatenation (Practice Lecture slides 6p)\n",
        "        x=torch.cat([conv1,x],dim=1)\n",
        "        x = self.convUp1(x)\n",
        "\n",
        "        out = self.convUp_fin(x)\n",
        "\n",
        "        return out\n"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbhEntno0t3r",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "057bdc89-535d-4a22-dded-87387b1e8d22"
      },
      "source": [
        "#from datasets import Loader\n",
        "import torchvision.transforms as transforms\n",
        "import PIL.Image as PIL\n",
        "#from modules import *\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "# from resnet_encoder_unet import *\n",
        "\n",
        "###########################################################################\n",
        "# Question 4 : Implement the main code.\n",
        "# Understand loading model, saving model, model initialization,\n",
        "# setting optimizer and loss in Practice Lecture 14, and fill in the blanks.(20 points)\n",
        "\n",
        "# batch size\n",
        "batch_size = 16\n",
        "learning_rate = 0.001\n",
        "\n",
        "# VOC2012 data directory\n",
        "data_dir = voc_dir\n",
        "resize_size = 256\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize([resize_size,resize_size], PIL.NEAREST),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "print(\"trainset\")\n",
        "trainset = Loader(data_dir, flag ='train', resize = resize_size, transforms = transforms)\n",
        "print(\"valset\")\n",
        "valset = Loader(data_dir, flag = 'val', resize = resize_size, transforms = transforms)\n",
        "\n",
        "print(\"tainLoader\")\n",
        "trainLoader = DataLoader(trainset, batch_size = batch_size, shuffle=True)\n",
        "print(\"valLoader\")\n",
        "validLoader = DataLoader(valset, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "##### fill in here #####\n",
        "##### Hint : Initialize the model (Options : Unet, resnet_encoder_unet)\n",
        "model = Unet(3,22)\n",
        "\n",
        "# Loss Function\n",
        "##### fill in here -> hint : set the loss function #####\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "##### fill in here -> hint : set the Optimizer #####\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n",
        "\n",
        "# parameters\n",
        "epochs = 1\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "##### fill in here #####\n",
        "##### Hint : load the model parameter, which is given\n",
        "PATH = '/content/drive/My Drive/opensw/trained_model/UNet_trained_model.pth'\n",
        "checkpoint = torch.load(PATH,map_location=torch.device('cuda:0'))\n",
        "model.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "# Train\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "date = now.strftime('%Y-%m-%d(%H:%M)')\n",
        "def createFolder(dir):\n",
        "    try:\n",
        "        if not os.path.exists(dir):\n",
        "            os.makedirs(dir)\n",
        "    except OSError:\n",
        "        print('Error: Creating directory. ' + dir)\n",
        "\n",
        "result_save_dir = '/content/drive/My Drive/opensw/history/result'+date+'/'\n",
        "createFolder(result_save_dir)\n",
        "predict_save_dir = result_save_dir + 'predicted/'\n",
        "createFolder(predict_save_dir)\n",
        "\n",
        "history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[]}\n",
        "\n",
        "print(\"Training\")\n",
        "\n",
        "savepath1 = \"/content/drive/My Drive/opensw/output/model\" + date + '/'\n",
        "createFolder(savepath1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_model(trainLoader, model, criterion, optimizer, scheduler, device)\n",
        "    train_acc, train_loss = get_loss_train(model, trainLoader, criterion, device)\n",
        "    print(\"epoch\", epoch + 1, \"train loss : \", train_loss, \"train acc : \", train_acc)\n",
        "\n",
        "    predict_save_folder = predict_save_dir + 'epoch' + str(epoch) + '/'\n",
        "    createFolder(predict_save_folder)\n",
        "    val_acc, val_loss = val_model(model, validLoader, criterion, device, predict_save_folder)\n",
        "    print(\"epoch\", epoch + 1, \"val loss : \", val_loss, \"val acc : \", val_acc)\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    if epoch % 4 == 0:\n",
        "        savepath2 = savepath1 + str(epoch) + \".pth\"\n",
        "        torch.save(model.state_dict(), savepath2)\n",
        "        ##### fill in here #####\n",
        "        ##### Hint : save the model parameter\n",
        "\n",
        "print('Finish Training')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(range(epoch+1), history['train_loss'], label='Loss', color='red')\n",
        "plt.plot(range(epoch+1), history['val_loss'], label='Loss', color='blue')\n",
        "\n",
        "plt.title('Loss history')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "# plt.show\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(range(epoch+1), history['train_acc'], label='Accuracy', color='red')\n",
        "plt.plot(range(epoch+1), history['val_acc'], label='Accuracy', color='blue')\n",
        "\n",
        "plt.title('Accuracy history')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.savefig(result_save_dir+'result')\n",
        "\n",
        "print(\"Fin\")\n",
        "\n"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainset\n",
            "valset\n",
            "tainLoader\n",
            "valLoader\n",
            "Training\n",
            "Step [10/146], Train_Loss: 3.0682\n",
            "Step [20/146], Train_Loss: 2.2391\n",
            "Step [30/146], Train_Loss: 1.8963\n",
            "Step [40/146], Train_Loss: 1.7169\n",
            "Step [50/146], Train_Loss: 1.5854\n",
            "Step [60/146], Train_Loss: 1.4872\n",
            "Step [70/146], Train_Loss: 1.4157\n",
            "Step [80/146], Train_Loss: 1.3527\n",
            "Step [90/146], Train_Loss: 1.2965\n",
            "Step [100/146], Train_Loss: 1.2569\n",
            "Step [110/146], Train_Loss: 1.2186\n",
            "Step [120/146], Train_Loss: 1.1892\n",
            "Step [130/146], Train_Loss: 1.1611\n",
            "Step [140/146], Train_Loss: 1.1354\n",
            "Step [10/146], Loss: 0.5797\n",
            "Step [20/146], Loss: 0.5618\n",
            "Step [30/146], Loss: 0.5605\n",
            "Step [40/146], Loss: 0.5550\n",
            "Step [50/146], Loss: 0.5593\n",
            "Step [60/146], Loss: 0.5618\n",
            "Step [70/146], Loss: 0.5622\n",
            "Step [80/146], Loss: 0.5675\n",
            "Step [90/146], Loss: 0.5703\n",
            "Step [100/146], Loss: 0.5699\n",
            "Step [110/146], Loss: 0.5684\n",
            "Step [120/146], Loss: 0.5654\n",
            "Step [130/146], Loss: 0.5649\n",
            "Step [140/146], Loss: 0.5669\n",
            "epoch 1 train loss :  0.5693771851389375 train acc :  0.8298488423358162\n",
            "Step [10/37], Val_Loss: 0.8658\n",
            "Step [20/37], Val_Loss: 0.8858\n",
            "Step [30/37], Val_Loss: 0.9057\n",
            "epoch 1 val loss :  0.9209931057852667 val acc :  0.7615844193879548\n",
            "Finish Training\n",
            "Fin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6RHDydT_ecD8",
        "colab_type": "text"
      },
      "source": [
        "# resnet_encoder_unet_skeleton"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HVaiuqAFI_K8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "5109360f-d18f-4112-c7b5-22b28086614b"
      },
      "source": [
        "print(torch.__version__)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.5.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qF-RaOqzGnMR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "6107cee0-1392-425a-8bb1-fe951a6656eb"
      },
      "source": [
        "!pip install torch"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOl40VZGi7tN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "# resnet = torchvision.models.resnet.resnet50(pretrained=True)\n",
        "\n",
        "# 1x1 convolution\n",
        "def conv1x1(in_channels, out_channels, stride, padding):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, padding=padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "# 3x3 convolution\n",
        "def conv3x3(in_channels, out_channels, stride, padding):\n",
        "    model = nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=padding),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "###########################################################################\n",
        "# Code overlaps with previous assignments : Implement the \"bottle neck building block\" part.\n",
        "# Hint : Think about difference between downsample True and False. How we make the difference by code?\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_channels, middle_channels, out_channels, downsample=False):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.downsample = downsample\n",
        "\n",
        "        if self.downsample:\n",
        "            self.layer = nn.Sequential(\n",
        "                ##########################################\n",
        "                ############## fill in here\n",
        "                # Hint : use these functions (conv1x1, conv3x3)\n",
        "                #########################################\n",
        "                conv1x1(in_channels,middle_channels,2,0),\n",
        "                conv3x3(middle_channels,middle_channels,1,1),\n",
        "                conv1x1(middle_channels,out_channels,1,0)\n",
        "            )\n",
        "            self.downsize = conv1x1(in_channels, out_channels, 2, 0)\n",
        "\n",
        "        else:\n",
        "            self.layer = nn.Sequential(\n",
        "                ##########################################\n",
        "                ############# fill in here\n",
        "                #########################################\n",
        "                conv1x1(in_channels,middle_channels,1,0),\n",
        "                conv3x3(middle_channels,middle_channels,1,1),\n",
        "                conv1x1(middle_channels,out_channels,1,0)\n",
        "            )\n",
        "            self.make_equal_channel = conv1x1(in_channels, out_channels, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.downsample:\n",
        "            out = self.layer(x)\n",
        "            x = self.downsize(x)\n",
        "            return out + x\n",
        "        else:\n",
        "            out = self.layer(x)\n",
        "            if x.size() is not out.size():\n",
        "                x = self.make_equal_channel(x)\n",
        "            return out + x\n",
        "\n",
        "def conv(in_channels, out_channels):\n",
        "    return nn.Sequential(\n",
        "        nn.Conv2d(in_channels, out_channels, 3, padding=1),  # 3은 kernel size\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True),  # inplace가 TRUE인 것은 할당 없이 바로 적용하겠다.\n",
        "        nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
        "        nn.BatchNorm2d(out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "class UNetWithResnet50Encoder(nn.Module):\n",
        "    def __init__(self, n_classes=22):\n",
        "        super().__init__()\n",
        "        self.n_classes = n_classes\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3), # Code overlaps with previous assignments\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True)#,\n",
        "        )\n",
        "        self.pool = nn.MaxPool2d(3, 2, 1, return_indices=True)\n",
        "\n",
        "        self.layer2 = nn.Sequential(\n",
        "            ResidualBlock(64, 64, 256, downsample=False),\n",
        "            ResidualBlock(256, 64, 256, downsample=False),\n",
        "            ResidualBlock(256, 64, 256, downsample=True)\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            ResidualBlock(256,128,512, downsample=False),\n",
        "            ResidualBlock(512,128,512, downsample=False),\n",
        "            ResidualBlock(512,128,512, downsample=False),\n",
        "            ResidualBlock(512,128,512, downsample=False),\n",
        "        )\n",
        "        self.bridge = conv(512, 512)\n",
        "        self.UnetConv1 = conv(512, 256)\n",
        "        self.UpConv1 = nn.Conv2d(512, 256, 3, padding=1)\n",
        "\n",
        "        self.upconv2_1 = nn.ConvTranspose2d(256, 256, 3, 2, 1)\n",
        "        self.upconv2_2 = nn.Conv2d(256, 64, 3, padding=1)\n",
        "\n",
        "        self.unpool = nn.MaxUnpool2d(3, 2, 1)\n",
        "        self.UnetConv2_1 = nn.ConvTranspose2d(64, 64, 3, 2, 1)\n",
        "        self.UnetConv2_2 = nn.ConvTranspose2d(128, 128, 3, 2, 1)\n",
        "        self.UnetConv2_3 = nn.Conv2d(128, 64, 3, padding=1)\n",
        "\n",
        "        self.UnetConv3 = nn.Conv2d(64, self.n_classes, kernel_size=1, stride=1)\n",
        "\n",
        "    ###########################################################################\n",
        "    # Question 2 : Implement the forward function of Resnet_encoder_UNet.\n",
        "    # Understand ResNet, UNet architecture and fill in the blanks below. (20 points)\n",
        "    def forward(self, x, with_output_feature_map=False): #256\n",
        "\n",
        "        out1 = self.layer1(x)\n",
        "        out1, indices = self.pool(out1)\n",
        "        out2 = self.layer2(out1)\n",
        "        out3 = self.layer3(out2)\n",
        "        x = self.bridge(out3) # bridge\n",
        "        x = self.UpConv1(x)\n",
        "        x = torch.cat([x, out2], dim=1)\n",
        "        x = self.UnetConv1(x)\n",
        "        x = self.upconv2_1(x, output_size=torch.Size([x.size(0),256,64,64]))\n",
        "        x = self.upconv2_2(x)\n",
        "        x = torch.cat([x, out1], dim=1)\n",
        "        x = self.UnetConv2_2(x, output_size=torch.Size([x.size(0), 128, 128, 128]))\n",
        "        x = self.UnetConv2_2(x, output_size=torch.Size([x.size(0), 128, 256, 256]))\n",
        "        x = self.UnetConv2_3(x)\n",
        "        x = self.UnetConv3(x)\n",
        "        return x\n"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CMrt2NsAwVy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a33da546-62c8-4a25-993a-69ad1cbc9e54"
      },
      "source": [
        "#from datasets import Loader\n",
        "import torchvision.transforms as transforms\n",
        "import PIL.Image as PIL\n",
        "# from modules import *\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "#from resnet_encoder_unet import *\n",
        "\n",
        "###########################################################################\n",
        "# Question 4 : Implement the main code.\n",
        "# Understand loading model, saving model, model initialization,\n",
        "# setting optimizer and loss in Practice Lecture 14, and fill in the blanks.(20 points)\n",
        "\n",
        "# batch size\n",
        "batch_size = 16\n",
        "learning_rate = 0.001\n",
        "\n",
        "# VOC2012 data directory\n",
        "data_dir = voc_dir\n",
        "resize_size = 256\n",
        "\n",
        "transforms = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize([resize_size,resize_size], PIL.NEAREST),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "print(\"trainset\")\n",
        "trainset = Loader(data_dir, flag ='train', resize = resize_size, transforms = transforms)\n",
        "print(\"valset\")\n",
        "valset = Loader(data_dir, flag = 'val', resize = resize_size, transforms = transforms)\n",
        "\n",
        "print(\"tainLoader\")\n",
        "trainLoader = DataLoader(trainset, batch_size = batch_size, shuffle=True)\n",
        "print(\"valLoader\")\n",
        "validLoader = DataLoader(valset, batch_size = batch_size, shuffle=True)\n",
        "\n",
        "##### fill in here #####\n",
        "##### Hint : Initialize the model (Options : Unet, resnet_encoder_unet)\n",
        "model = UNetWithResnet50Encoder()\n",
        "###############################################################################\n",
        "\n",
        "# Loss Function\n",
        "##### fill in here -> hint : set the loss function #####\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "##### fill in here -> hint : set the Optimizer #####\n",
        "optimizer = torch.optim.Adam(model.parameters(),lr=learning_rate)\n",
        "scheduler = StepLR(optimizer, step_size=4, gamma=0.1)\n",
        "\n",
        "# parameters\n",
        "epochs = 1\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "##### fill in here #####\n",
        "##### Hint : load the model parameter, which is given\n",
        "\n",
        "PATH = '/content/drive/My Drive/opensw/trained_model/resnet_unet_trained_model.pth'\n",
        "checkpoint = torch.load(PATH,map_location=torch.device('cuda:0'))\n",
        "model.load_state_dict(checkpoint, strict=False)\n",
        "\n",
        "# Train\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "now = datetime.now()\n",
        "date = now.strftime('%Y-%m-%d(%H:%M)')\n",
        "def createFolder(dir):\n",
        "    try:\n",
        "        if not os.path.exists(dir):\n",
        "            os.makedirs(dir)\n",
        "    except OSError:\n",
        "        print('Error: Creating directory. ' + dir)\n",
        "\n",
        "result_save_dir = '/content/drive/My Drive/opensw/history/result'+date+'/'\n",
        "createFolder(result_save_dir)\n",
        "predict_save_dir = result_save_dir + 'predicted/'\n",
        "createFolder(predict_save_dir)\n",
        "\n",
        "history = {'train_loss':[], 'train_acc':[], 'val_loss':[], 'val_acc':[]}\n",
        "\n",
        "print(\"Training\")\n",
        "\n",
        "savepath1 = \"/content/drive/My Drive/opensw/output/model\" + date + '/'\n",
        "createFolder(savepath1)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "\n",
        "    train_model(trainLoader, model, criterion, optimizer, scheduler, device)\n",
        "    train_acc, train_loss = get_loss_train(model, trainLoader, criterion, device)\n",
        "    print(\"epoch\", epoch + 1, \"train loss : \", train_loss, \"train acc : \", train_acc)\n",
        "\n",
        "    predict_save_folder = predict_save_dir + 'epoch' + str(epoch) + '/'\n",
        "    createFolder(predict_save_folder)\n",
        "    val_acc, val_loss = val_model(model, validLoader, criterion, device, predict_save_folder)\n",
        "    print(\"epoch\", epoch + 1, \"val loss : \", val_loss, \"val acc : \", val_acc)\n",
        "\n",
        "    history['train_loss'].append(train_loss)\n",
        "    history['train_acc'].append(train_acc)\n",
        "    history['val_loss'].append(val_loss)\n",
        "    history['val_acc'].append(val_acc)\n",
        "\n",
        "    if epoch % 4 == 0:\n",
        "        savepath2 = savepath1 + str(epoch) + \".pth\"\n",
        "        torch.save(model.state_dict(), savepath2)\n",
        "        ##### fill in here #####\n",
        "        ##### Hint : save the model parameter\n",
        "\n",
        "print('Finish Training')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.subplot(2,1,1)\n",
        "plt.plot(range(epoch+1), history['train_loss'], label='Loss', color='red')\n",
        "plt.plot(range(epoch+1), history['val_loss'], label='Loss', color='blue')\n",
        "\n",
        "plt.title('Loss history')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')\n",
        "# plt.show\n",
        "\n",
        "plt.subplot(2,1,2)\n",
        "plt.plot(range(epoch+1), history['train_acc'], label='Accuracy', color='red')\n",
        "plt.plot(range(epoch+1), history['val_acc'], label='Accuracy', color='blue')\n",
        "\n",
        "plt.title('Accuracy history')\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('accuracy')\n",
        "plt.savefig(result_save_dir+'result')\n",
        "\n",
        "print(\"Fin\")\n",
        "\n"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainset\n",
            "valset\n",
            "tainLoader\n",
            "valLoader\n",
            "Training\n",
            "Step [10/146], Train_Loss: 1.2247\n",
            "Step [20/146], Train_Loss: 1.0141\n",
            "Step [30/146], Train_Loss: 0.9166\n",
            "Step [40/146], Train_Loss: 0.8594\n",
            "Step [50/146], Train_Loss: 0.8276\n",
            "Step [60/146], Train_Loss: 0.7989\n",
            "Step [70/146], Train_Loss: 0.7738\n",
            "Step [80/146], Train_Loss: 0.7778\n",
            "Step [90/146], Train_Loss: 0.7640\n",
            "Step [100/146], Train_Loss: 0.7524\n",
            "Step [110/146], Train_Loss: 0.7412\n",
            "Step [120/146], Train_Loss: 0.7435\n",
            "Step [130/146], Train_Loss: 0.7484\n",
            "Step [140/146], Train_Loss: 0.7390\n",
            "Step [10/146], Loss: 0.6279\n",
            "Step [20/146], Loss: 0.6604\n",
            "Step [30/146], Loss: 0.6743\n",
            "Step [40/146], Loss: 0.6654\n",
            "Step [50/146], Loss: 0.6687\n",
            "Step [60/146], Loss: 0.6696\n",
            "Step [70/146], Loss: 0.6746\n",
            "Step [80/146], Loss: 0.6749\n",
            "Step [90/146], Loss: 0.6755\n",
            "Step [100/146], Loss: 0.6664\n",
            "Step [110/146], Loss: 0.6623\n",
            "Step [120/146], Loss: 0.6580\n",
            "Step [130/146], Loss: 0.6578\n",
            "Step [140/146], Loss: 0.6567\n",
            "epoch 1 train loss :  0.6581405729871906 train acc :  0.8253479894635687\n",
            "Step [10/37], Val_Loss: 1.1446\n",
            "Step [20/37], Val_Loss: 1.0755\n",
            "Step [30/37], Val_Loss: 1.1241\n",
            "epoch 1 val loss :  1.0994150976876955 val acc :  0.7235318003474055\n",
            "Finish Training\n",
            "Fin\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}